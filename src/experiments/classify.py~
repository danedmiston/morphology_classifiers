from sklearn.metrics import f1_score
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from conll.lexicon import Lexicon
from utils.loader import *
from sklearn.cluster import *
from utils.book_keeping import *
from sklearn.model_selection import train_test_split
from torch import nn
import torch.nn.functional as F
from skorch import NeuralNetClassifier
from skorch.callbacks import Checkpoint, EarlyStopping
from tqdm import tqdm


class Classify():
    def __init__(self, language, feature):
        self.language = language
        self.feature = feature

        self.num_classes = len(Features[self.language][feature])
        self.point_clouds = load_point_clouds(language=self.language, feature=self.feature)

        self.f2l, self.l2f = feat2label(self.language, self.feature)

        self.lexicon = Lexicon(self.language)
        self.features = Features[self.language]

        self.ambiguity_dict = {}
        

    def evaluate_by_ambiguity(self, quadruples):
        """
        Takes a list of triples of the form [(word, ambiguity, y_hat, y)]
        Partitions dataset by how ambiguous words are w.r.t. relevant feature

        Tests accuracy of these subsets. We want to see if performance
        is correlated with how ambiguous words are
        """
        scores = []
        possible_values = len(self.features[self.feature])
        for i in range(1, possible_values+1):
            if i == 1:
                # Some entries not listed for a given feature; treat these as ambiguity 1
                subset = [item for item in quadruples if item[1] == 0 or item[1] == 1]
            else:
                subset = [item for item in quadruples if item[1] == i]
            if len(subset) > 0:
                y_hat = [item[2] for item in subset]
                y = [item[3] for item in subset]
                scores.append((i, f1_score(y, y_hat, average="weighted")))
        return(scores)
            
        
        
class Clusterer(Classify):
    def __init__(self, language, feature, method=KMeans):
        super().__init__(language, feature)
        self.method = method
        

    def cluster_by_layer(self, layer, average="weighted"):
        # This first part deals with calculating f1 score
        clustering = self.method(n_clusters=self.num_classes, random_state=42)
        clustering.fit(np.stack(self.point_clouds[layer]["Vector"].values, axis=0))
        ground_truth = [self.f2l[list(item)[0]] for item in self.point_clouds[layer]["Class"]]
        f1 = f1_score(ground_truth, clustering.labels_, average=average)

        # This second part concerns calculating accuracy for words which
        # are different levels of ambiguous
        # This let's us ask if there is a correlation between how ambiguous
        # a word is w.r.t. some feature, and how well a model can predict
        # the word's value in context
        words = [word for word in self.point_clouds[layer]["Word"]]
        ambis = [len(ambi) for ambi in self.point_clouds[layer]["Ambiguity"]]
        quads = [(words[i], ambis[i], clustering.labels_[i], ground_truth[i])
                 for i in range(len(words))]
        amb_scores = self.evaluate_by_ambiguity(quads)
        #            
        return(f1, amb_scores)

    def run_cluster_experiment(self, average="weighted"):
        f1s = []
        amb_scores_list = []
        with open(Cluster_Results[self.language] + self.feature + ".txt", "w") as fout:
            for i in range(13):
                f1, amb_scores = self.cluster_by_layer(i, average=average)
                f1s.append(f1)
                amb_scores_list.append(amb_scores)
                print(f1, amb_scores, "\n\n")

                fout.write("f1 for layer " + str(i) + ": " + str(f1) + "\n")
                fout.write("Ambiguity f1 for layer " + str(i) + ": " + str(amb_scores) + "\n\n\n")
            print("Average f1:", np.mean(f1s))
            fout.write("Average f1 for all layers:" + str(np.mean(f1s)) + "\n")
            possible_values = len(self.features[self.feature])
            for i in range(possible_values):
                try:
                    scores = np.stack([item[i][1] for item in amb_scores_list], axis=0)
                    mean = np.mean(scores) 
                    print("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                    fout.write("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                except:
                    continue

class Linear(Classify):
    def __init__(self, language, feature):
        super().__init__(language, feature)

        self.train_set = {}
        self.test_set = {}

        print("Preparing data sets...")
        

        for layer in range(13):
            
            self.train_set[layer] = {}
            self.test_set[layer] = {}

            vectors = self.point_clouds[layer][["Word", "Ambiguity", "Vector"]]
            ground_truth = [self.f2l[list(item)[0]] for item in self.point_clouds[layer]["Class"]]
            X_train, X_test, y_train, y_test = train_test_split(vectors, ground_truth,
                                                                test_size=0.15, random_state=42)
            self.train_set[layer]["X"] = X_train
            self.train_set[layer]["y"] = y_train

            self.test_set[layer]["X"] = X_test
            self.test_set[layer]["y"] = y_test

            
    def linear_by_layer(self, layer, average="weighted"):
        # Training
        X_train = np.stack(self.train_set[layer]["X"]["Vector"].values, axis=0)
        y_train = np.stack(self.train_set[layer]["y"], axis=0)
        clf = LogisticRegression(random_state=42, max_iter=2500)
        clf.fit(X_train, y_train)
        # Testing
        X_test = np.stack(self.test_set[layer]["X"]["Vector"].values, axis=0)
        y_test = np.stack(self.test_set[layer]["y"], axis=0)
        y_hat = clf.predict(X_test)
        f1 = f1_score(y_test, y_hat, average=average)
        
        words = [word for word in self.test_set[layer]["X"]["Word"]]
        ambis = [len(ambi) for ambi in self.test_set[layer]["X"]["Ambiguity"]]
        quads = [(words[i], ambis[i], y_hat[i], y_test[i])
                 for i in range(len(words))]
        amb_scores = self.evaluate_by_ambiguity(quads)

        
        return(f1, amb_scores)
        
    def run_linear_experiment(self, average="weighted"):
        f1s = []
        amb_scores_list = []
        with open(Linear_Results[self.language] + self.feature + ".txt", "w") as fout:
            for i in range(13):
                f1, amb_scores = self.linear_by_layer(i, average=average)
                f1s.append(f1)
                amb_scores_list.append(amb_scores)
                print(f1, amb_scores, "\n\n")

                fout.write("f1 for layer " + str(i) + ": " + str(f1) + "\n")
                fout.write("Ambiguity f1 for layer " + str(i) + ": " + str(amb_scores) + "\n\n\n")
            print("Average f1:", np.mean(f1s))
            fout.write("Average f1 for all layers:" + str(np.mean(f1s)) + "\n")
            possible_values = len(self.features[self.feature])
            for i in range(possible_values):
                try:
                    scores = np.stack([item[i][1] for item in amb_scores_list], axis=0)
                    mean = np.mean(scores) 
                    print("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                    fout.write("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                except:
                    continue


class NonLinear(Linear):
    def __init__(self, language, feature, hidden=768, activation=F.relu):
        super().__init__(language, feature)

        self.hidden = hidden
        self.activation = activation

        class TwoLayerNN(nn.Module):
            def __init__(self, hidden, num_classes, activation):
                super(TwoLayerNN, self).__init__()

                self.hidden = hidden
                self.num_classes = num_classes
                self.activation = activation

            
                self.Layer1 = nn.Linear(768, self.hidden, bias=True)
                self.Layer2 = nn.Linear(self.hidden, self.hidden, bias=True)
                self.Output = nn.Linear(self.hidden, self.num_classes, bias=True)

            def forward(self, X):
                z1 = self.Layer1(X)
                a1 = self.activation(z1)
                z2 = self.Layer2(a1)
                a2 = self.activation(z2)
                y_hat = F.softmax(a2)
                return(y_hat)       

        self.monitor = lambda net: all(net.history[-1, ("train_loss_best", "valid_loss_best")])
        self.net = NeuralNetClassifier(module=TwoLayerNN,
                                       module__hidden=self.hidden,
                                       module__num_classes=self.num_classes,
                                       module__activation=self.activation,
                                       max_epochs=2500,
                                       lr=0.1, iterator_train__shuffle=True,
                                       callbacks=[Checkpoint(monitor=self.monitor),
                                                  EarlyStopping(patience=10,
                                                                threshold=0.00001)])
        
    def nonlinear_by_layer(self, layer, average="weighted"):
        X_train = np.stack(self.train_set[layer]["X"]["Vector"].values, axis=0)
        y_train = np.stack(self.train_set[layer]["y"], axis=0)
        self.net.fit(X_train, y_train)
        X_test = np.stack(self.test_set[layer]["X"]["Vector"].values, axis=0)
        y_test = np.stack(self.test_set[layer]["y"], axis=0)
        y_hat = self.net.predict(X_test)
        f1 = f1_score(y_test, y_hat, average=average)

        words = [word for word in self.test_set[layer]["X"]["Word"]]
        ambis = [len(ambi) for ambi in self.test_set[layer]["X"]["Ambiguity"]]
        quads = [(words[i], ambis[i], y_hat[i], y_test[i])
                 for i in range(len(words))]
        amb_scores = self.evaluate_by_ambiguity(quads)

        return(f1, amb_scores)

    
    def run_nonlinear_experiment(self, average="weighted"):
        f1s = []
        amb_scores_list = []
        with open(NonLinear_Results[self.language] + self.feature + ".txt", "w") as fout:
            for i in range(13):
                f1, amb_scores = self.nonlinear_by_layer(i, average=average)
                f1s.append(f1)
                amb_scores_list.append(amb_scores)
                print(f1, amb_scores, "\n\n")

                fout.write("f1 for layer " + str(i) + ": " + str(f1) + "\n")
                fout.write("Ambiguity f1 for layer " + str(i) + ": " + str(amb_scores) + "\n\n\n")
            print("Average f1:", np.mean(f1s))
            fout.write("Average f1 for all layers:" + str(np.mean(f1s)) + "\n")
            possible_values = len(self.features[self.feature])
            for i in range(possible_values):
                try:
                    scores = np.stack([item[i][1] for item in amb_scores_list], axis=0)
                    mean = np.mean(scores) 
                    print("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                    fout.write("Average f1 for ambiguity=" + str(i+1) + ": " + str(mean) + "\n")
                except:
                    continue


    
    
